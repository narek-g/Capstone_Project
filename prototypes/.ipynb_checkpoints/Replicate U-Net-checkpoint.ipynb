{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Submission: Replicate Research Paper\n",
    "Created December 27, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net (http://www.cs.cmu.edu/~jeanoh/16-785/papers/ronnenberger-miccai2015-u-net.pdf), a convolutional neural network developed for biological image segmentation. \n",
    "<P> \n",
    "While U-Net is not specifically for clothing segmentation, there are public projects applying the model for said applications, in addition to updated models built ontop of U-Net. \n",
    "<P> \n",
    "In this notebook, we will apply the model developed by U-Net for basic segmentation. As the capstone project will explore model development (along with other components of the architectual pipeline), we will not be training the model yet - only loading the trained model and testing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Note: Pre-processing steps not shown in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/Users/narekgeghamyan/Classes/MLE_bootcamp/Clothing-Segmentation')\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, \\\n",
    "    MaxPooling2D, Input, Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(object):\n",
    "    def __init__(self, img_shape, num_of_class, actf = 'relu',\n",
    "        learning_rate = 0.001,  drop_rate = 0.5, do_batch_norm = False, do_drop = False):\n",
    "\n",
    "        '''\n",
    "        Arguments :\n",
    "\n",
    "        img_shape - shape of input image (64, 64, 1)\n",
    "        actf - activation function for network training\n",
    "        learning_rate - learning rate for training\n",
    "        drop_rate - dropout rate\n",
    "        do_batch_norm - whether to run for batchnormalization\n",
    "        do_drop - whether to run for dropout\n",
    "        '''\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.actf = actf\n",
    "        self.img_shape = img_shape\n",
    "        self.num_of_class = num_of_class\n",
    "        self.drop_rate = drop_rate\n",
    "        self.do_batch_norm = do_batch_norm\n",
    "        self.do_drop = do_drop\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # encoding block(conv - conv - pool)\n",
    "    def enc_conv_block(self, inputs, feature_maps, filter_size = (3, 3),\n",
    "                           conv_strides = 1, pooling_filter_size = (2, 2), pooling_strides = (2, 2)):\n",
    "        conv1 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n",
    "                           padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv2 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n",
    "                           padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        pool = MaxPooling2D(pooling_filter_size, strides = pooling_strides)(conv2)\n",
    "\n",
    "        return pool, conv2\n",
    "\n",
    "    # decoding block(concat - upconv - upconv)\n",
    "    def dec_conv_block(self, inputs, merge_inputs, feature_maps, filter_size = (3, 3), conv_strides = 1,\n",
    "                           up_conv_strides = (2, 2)):\n",
    "\n",
    "        merge = Concatenate(axis = 3)([Conv2DTranspose(feature_maps, filter_size,\n",
    "                                                       activation = self.actf, strides = up_conv_strides, \n",
    "                                                       kernel_initializer = 'he_normal',\n",
    "                                                       padding = 'same')(inputs), merge_inputs])\n",
    "\n",
    "        conv1 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n",
    "                           padding = 'same', kernel_initializer = 'he_normal')(merge)\n",
    "        conv2 = Conv2D(feature_maps , filter_size , activation = self.actf, strides = conv_strides,\n",
    "                           padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    # encoder\n",
    "    def encoding_path(self, inputs):\n",
    "\n",
    "        enc_conv1, concat1 = self.enc_conv_block(inputs, 64)\n",
    "        enc_conv2, concat2 = self.enc_conv_block(enc_conv1, 128)\n",
    "        enc_conv3, concat3 = self.enc_conv_block(enc_conv2, 256)\n",
    "        enc_conv4, concat4 = self.enc_conv_block(enc_conv3, 512)\n",
    "\n",
    "        return concat1, concat2, concat3, concat4, enc_conv4\n",
    "\n",
    "    # decoder\n",
    "    def decoding_path(self, dec_inputs, concat1, concat2, concat3, concat4):\n",
    "\n",
    "        dec_conv1 = self.dec_conv_block(dec_inputs, concat4, 512)\n",
    "        dec_conv2 = self.dec_conv_block(dec_conv1, concat3, 256)\n",
    "        dec_conv3 = self.dec_conv_block(dec_conv2, concat2, 128)\n",
    "        dec_conv4 = self.dec_conv_block(dec_conv3, concat1, 64)\n",
    "\n",
    "        return dec_conv4\n",
    "    # build network\n",
    "    def build_model(self):\n",
    "        inputs = Input(self.img_shape)\n",
    "\n",
    "        # Contracting path\n",
    "        concat1, concat2, concat3, concat4, enc_path = self.encoding_path(inputs)\n",
    "\n",
    "        # middle path\n",
    "        mid_path1 = Conv2D(1024, (3,3), activation = self.actf, padding = 'same', \n",
    "                           kernel_initializer = 'he_normal')(enc_path)\n",
    "        mid_path1 = Dropout(self.drop_rate)(mid_path1)\n",
    "        mid_path2 = Conv2D(1024, (3,3), activation = self.actf, padding = 'same', \n",
    "                           kernel_initializer = 'he_normal')(mid_path1)\n",
    "        mid_path2 = Dropout(self.drop_rate)(mid_path2)\n",
    "\n",
    "        # Expanding path\n",
    "        dec_path = self.decoding_path(mid_path2, concat1, concat2, concat3, concat4)\n",
    "        segmented = Conv2D(self.num_of_class, (1,1), activation = self.actf, padding = 'same', \n",
    "                           kernel_initializer = 'he_normal')(dec_path)\n",
    "        segmented = Activation('softmax')(segmented)\n",
    "\n",
    "        model = Model(inputs = inputs, outputs = segmented)\n",
    "        model.compile(optimizer = Adam(learning_rate = self.learning_rate),\n",
    "                          loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # train model\n",
    "    def train(self, X_train, Y_train, epoch = 10, batch_size = 32, val_split = 0.2, shuffle = True):\n",
    "\n",
    "        self.history = self.model.fit(X_train, Y_train, validation_split = val_split,\n",
    "                                          epochs = epoch, batch_size = batch_size, shuffle =  shuffle)\n",
    "        return self.history\n",
    "\n",
    "    # train with data augmentation\n",
    "    def train_generator(self, x_train, y_train, x_test, y_test, name_model, epoch = 10, \n",
    "                        batch_size = 32, val_split = 0.2, min_lr = 1e-06):\n",
    "\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            brightness_range=[0.7, 1.3]\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            rescale=1./255\n",
    "        )\n",
    "\n",
    "        train_gen = train_datagen.flow(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size = batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_gen = val_datagen.flow(\n",
    "            x_test,\n",
    "            y_test,\n",
    "            batch_size = batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        save_dir = './save_model/'\n",
    "        if not os.path.exists(save_dir): # if there is no exist, make the path\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        cb_checkpoint = ModelCheckpoint(save_dir + name_model + '.h5', \n",
    "                                        monitor = 'val_acc', save_best_only = True, verbose = 1)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor = 'val_acc',factor = 0.2, patience = 5, verbose = 1, min_lr = min_lr)\n",
    "\n",
    "        self.history = self.model.fit(train_gen,\n",
    "                                                validation_data=val_gen,\n",
    "                                                epochs=epoch,\n",
    "                                                callbacks=[cb_checkpoint, reduce_lr])\n",
    "        return self.history\n",
    "    # predict test data\n",
    "    def predict(self, X_test):\n",
    "        pred_classes = self.model.predict(X_test)\n",
    "\n",
    "        return pred_classes\n",
    "\n",
    "    # show architecture\n",
    "    def show_model(self):\n",
    "        return print(self.model.summary())\n",
    "\n",
    "    # reuse model\n",
    "    def saved_model_use(self, save_dir = None):\n",
    "        if save_dir == None:\n",
    "            return print('No path')\n",
    "\n",
    "        self.model.load_weights(save_dir)\n",
    "\n",
    "        return print(\"Loaded model from '{}'\".format(save_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 384, 256, 3) (900, 384, 256, 4)\n",
      "(100, 384, 256, 3) (100, 384, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "# IMG_HEIGHT = 384\n",
    "# IMG_WIDTH = 256\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# npy files created in preprocessing\n",
    "x_train = np.load('dataset/x_train.npy').astype(np.float32)\n",
    "x_test = np.load('dataset/x_test.npy').astype(np.float32)\n",
    "y_train = np.load('dataset/y_train_onehot.npy').astype(np.float32)\n",
    "y_test = np.load('dataset/y_test_onehot.npy').astype(np.float32)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 384, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 384, 256, 64  1792        ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 384, 256, 64  36928       ['conv2d_19[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 192, 128, 64  0          ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 192, 128, 12  73856       ['max_pooling2d_4[0][0]']        \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 192, 128, 12  147584      ['conv2d_21[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 96, 64, 128)  0          ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 96, 64, 256)  295168      ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 96, 64, 256)  590080      ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 48, 32, 256)  0          ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 48, 32, 512)  1180160     ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 48, 32, 512)  2359808     ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 24, 16, 512)  0          ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 24, 16, 1024  4719616     ['max_pooling2d_7[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 24, 16, 1024  0           ['conv2d_27[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 24, 16, 1024  9438208     ['dropout_2[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 24, 16, 1024  0           ['conv2d_28[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 48, 32, 512)  4719104    ['dropout_3[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 48, 32, 1024  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                )                                 'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 48, 32, 512)  4719104     ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 48, 32, 512)  2359808     ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 96, 64, 256)  1179904    ['conv2d_30[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 96, 64, 512)  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 96, 64, 256)  1179904     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 96, 64, 256)  590080      ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 192, 128, 12  295040     ['conv2d_32[0][0]']              \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 192, 128, 25  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                6)                                'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 192, 128, 12  295040      ['concatenate_6[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 192, 128, 12  147584      ['conv2d_33[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 384, 256, 64  73792      ['conv2d_34[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 384, 256, 12  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                8)                                'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 384, 256, 64  73792       ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 384, 256, 64  36928       ['conv2d_35[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 384, 256, 4)  260         ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 384, 256, 4)  0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,513,540\n",
      "Trainable params: 34,513,540\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = UNet(img_shape = x_train[0].shape, num_of_class = 4,learning_rate = 1e-3)\n",
    "model.show_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fa61c5f1b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fa61c5f1b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  4/113 [>.............................] - ETA: 40:19 - loss: 1.8481 - accuracy: 0.4861"
     ]
    }
   ],
   "source": [
    "history = model.train_generator(x_train, y_train, \n",
    "                                x_test, y_test, \n",
    "                                'UNet_model',\n",
    "                                epoch = 2,\n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
