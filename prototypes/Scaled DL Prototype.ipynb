{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary / Premise \n",
    "<P>The original U2-Net prototype is already developed for scale. In this notebook, I'll explore different methodologies for improvign computational efficiency and memory usage. We will test whether U2-Net already has an \"optimal\" model architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paper describing U2-Net: http://arxiv.org/abs/2005.09007 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P> While an adaptive model may be developed to account for various input image sizes (we want the same input size to classification layer), preprocessing the input image allows us to control the data dimentionality and thus both memory usage and computation time. \n",
    "<P> Furthermore, we'll explore whether training gray scale images produces the same learning as color (RGB) images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P> The convolution is a mathematical operation between the input and kernel, producing a feature map. \n",
    "    \n",
    "<P> The convolution used in CNN allows us imrove the learning process by utilizing different methodologies: Space Interactions, Parameter Sharing, and Equivariant Representation. \n",
    "    \n",
    "<P> Traditional neural net layers use matrix interactions descriing the interaction between each input unit and output unit - this is very costly. <b> Space interactions</b> occur when the kernel is smaller than the input. This allows for better memory requirements and statistical efficiency. The specifics of the kernel will depend greatly on the application, or problem to be solved, and require trial and error experimentation. \n",
    " \n",
    "<P> <b> Parameter sharing </b> allow for using the same parametsers for more than one function in a model. This allows for learning one set of parameters for every location, instead of a unique one - saving in memory storage requirements. Runtime in forward propagation would still be the same as the number of parameters still dictates the Order. \n",
    "    \n",
    "<P> Lastly, <b> equivariant representation </b> occurs due to parameter sharing. If a function is equivariant, it measns any changes to the input will result in the same changes to the output. This means the convolution may produce a timeline showing when different features appear in the input. Although not clearly necessary for working with images (where there is no time change), it may be useful if observing a video or series of images one after another. \n",
    "    \n",
    "<P> For out putposes, we will explore <b> space interactions</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P> Pooling is an operation which replaces the output of the net at a certain location with a summary statistic of the nearby outputs. Common examples are max pooing, averaging, or weighted averging. \n",
    "<P> In addition, pooling helps make representation approximately invariant to small translations of the input. This is a key feature as it may allow for more robustness! \n",
    "<P> Reserach into pooling suggests it's ideal when the model should care more if a feature is present rather than where it exactly is (ex. are there two eyes might be more important than where the eyes are if observing a face). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'[step-00000000] [time-384.013] [total_loss-10.594320]  [loss0-1.435154]' <P>\n",
    "'[step-00000010] [time-3758.220] [total_loss-7.376394]  [loss0-0.980869]' <P>\n",
    "'[step-00000020] [time-6955.041] [total_loss-7.588485]  [loss0-1.011537]' <P>\n",
    "'[step-00000030] [time-10167.091] [total_loss-6.323454]  [loss0-0.842958]'<P>\n",
    "'[step-00000040] [time-13401.983] [total_loss-7.955747]  [loss0-1.060730]'<P>\n",
    "'[step-00000050] [time-17110.690] [total_loss-7.161148]  [loss0-0.954787]'<P>\n",
    "'[step-00000060] [time-20505.491] [total_loss-7.635154]  [loss0-1.017976]'<P>\n",
    "'[step-00000070] [time-23772.851] [total_loss-6.705648]  [loss0-0.894021]'<P>\n",
    "'[step-00000080] [time-26995.722] [total_loss-7.189803]  [loss0-0.958591]'<P>\n",
    "'[step-00000090] [time-30212.212] [total_loss-7.688505]  [loss0-1.025093]'<P>\n",
    "'[step-00000100] [time-33438.927] [total_loss-7.031792]  [loss0-0.937536]'<P>\n",
    "'[step-00000110] [time-36675.915] [total_loss-7.350829]  [loss0-0.980091]'<P>\n",
    "'[step-00000120] [time-39932.529] [total_loss-6.820566]  [loss0-0.909410]'<P>\n",
    "'[step-00000130] [time-43169.930] [total_loss-7.534556]  [loss0-1.004617]'<P>\n",
    "'[step-00000140] [time-46361.347] [total_loss-7.627801]  [loss0-1.017001]'<P>\n",
    "'[step-00000150] [time-49562.756] [total_loss-7.434967]  [loss0-0.991422]'<P>\n",
    "'[step-00000160] [time-52809.173] [total_loss-7.066546]  [loss0-0.942169]'<P>\n",
    "'[step-00000170] [time-56027.030] [total_loss-8.452553]  [loss0-1.127077]'<P>\n",
    "'[step-00000180] [time-59248.706] [total_loss-7.313221]  [loss0-0.975026]'<P>\n",
    "'[step-00000190] [time-62445.289] [total_loss-7.010470]  [loss0-0.934705]'<P>\n",
    "'[step-00000200] [time-65761.253] [total_loss-7.158437]  [loss0-0.954448]'<P>\n",
    "'[step-00000210] [time-68971.391] [total_loss-7.144845]  [loss0-0.952658]'<P>\n",
    "'[step-00000220] [time-72174.184] [total_loss-7.141483]  [loss0-0.952409]'<P>\n",
    "'[step-00000230] [time-75414.522] [total_loss-7.723953]  [loss0-1.030127]'<P>\n",
    "'[step-00000240] [time-78594.097] [total_loss-8.183121]  [loss0-1.091378]'<P>\n",
    "'[step-00000250] [time-81829.286] [total_loss-6.667850]  [loss0-0.890392]'<P>\n",
    "'[step-00000260] [time-85054.862] [total_loss-6.777811]  [loss0-0.906160]'<P>\n",
    "'[step-00000270] [time-88266.792] [total_loss-6.655214]  [loss0-0.888538]'<P>\n",
    "'[step-00000280] [time-91456.050] [total_loss-7.583304]  [loss0-1.012199]'<P>\n",
    "'[step-00000290] [time-94735.386] [total_loss-6.851970]  [loss0-0.914760]'<P>\n",
    "'[step-00000300] [time-97953.898] [total_loss-7.832402]  [loss0-1.045591]'<P>\n",
    "'[step-00000310] [time-101262.281] [total_loss-8.148235]  [loss0-1.086490]'<P>\n",
    "'[step-00000320] [time-104539.575] [total_loss-6.737033]  [loss0-0.898477]'<P>\n",
    "'[step-00000330] [time-107819.430] [total_loss-6.602642]  [loss0-0.880328]'<P>\n",
    "'[step-00000340] [time-111101.472] [total_loss-6.547769]  [loss0-0.872228]'<P>\n",
    "'[step-00000350] [time-114478.304] [total_loss-7.141560]  [loss0-0.953669]'<P>\n",
    "'[step-00000360] [time-117702.056] [total_loss-8.116978]  [loss0-1.082147]'<P>\n",
    "'[step-00000370] [time-120938.673] [total_loss-8.280577]  [loss0-1.107240]'<P>\n",
    "'[step-00000380] [time-124160.494] [total_loss-6.763611]  [loss0-0.904343]'<P>\n",
    "'[step-00000390] [time-127380.327] [total_loss-7.472704]  [loss0-0.999767]'<P>\n",
    "'[step-00000400] [time-130598.306] [total_loss-6.929818]  [loss0-0.924546]'<P>\n",
    "'[step-00000410] [time-133844.223] [total_loss-7.290617]  [loss0-0.974550]'<P>\n",
    "'[step-00000420] [time-137116.103] [total_loss-6.888787]  [loss0-0.923319]'<P>\n",
    "'[step-00000430] [time-140378.372] [total_loss-7.330955]  [loss0-0.979761]'<P>\n",
    "'[step-00000440] [time-143634.113] [total_loss-7.024245]  [loss0-0.937685]'<P>\n",
    "'[step-00000450] [time-146881.778] [total_loss-7.558311]  [loss0-1.012852]'<P>\n",
    "'[step-00000460] [time-150170.404] [total_loss-6.878598]  [loss0-0.913327]'<P>\n",
    "'[step-00000470] [time-153422.072] [total_loss-6.782887]  [loss0-0.910447]'<P>\n",
    "'[step-00000480] [time-156645.466] [total_loss-6.813272]  [loss0-0.906368]'<P>\n",
    "'[step-00000490] [time-159914.782] [total_loss-6.746159]  [loss0-0.904712]'<P>\n",
    "'[step-00000500] [time-163119.333] [total_loss-7.198717]  [loss0-0.968617]'<P>\n",
    "'[step-00000510] [time-166336.234] [total_loss-7.772251]  [loss0-1.041406]'<P>\n",
    "'[step-00000520] [time-169581.251] [total_loss-6.794665]  [loss0-0.897550]'<P>\n",
    "'[step-00000530] [time-172801.820] [total_loss-8.061086]  [loss0-1.063441]'<P>\n",
    "'[step-00000540] [time-176007.773] [total_loss-7.325161]  [loss0-0.977373]'<P>\n",
    "'[step-00000550] [time-179264.299] [total_loss-7.316790]  [loss0-0.971028]'<P>\n",
    "'[step-00000560] [time-182484.914] [total_loss-7.532674]  [loss0-0.995228]'<P>\n",
    "'[step-00000570] [time-185743.406] [total_loss-7.366972]  [loss0-0.986793]'<P>\n",
    "'[step-00000580] [time-189078.117] [total_loss-6.943277]  [loss0-0.926439]'<P>\n",
    "'[step-00000590] [time-192336.940] [total_loss-6.964555]  [loss0-0.904869]'<P>\n",
    "'[step-00000600] [time-195533.755] [total_loss-7.620158]  [loss0-1.018462]'<P>\n",
    "'[step-00000610] [time-198883.275] [total_loss-7.981087]  [loss0-1.098930]'<P>\n",
    "'[step-00000620] [time-202129.450] [total_loss-7.089289]  [loss0-0.931358]'<P>\n",
    "'[step-00000630] [time-205441.837] [total_loss-6.402145]  [loss0-0.857902]'<P>\n",
    "'[step-00000640] [time-208665.715] [total_loss-8.050325]  [loss0-1.073728]'<P>\n",
    "'[step-00000650] [time-211976.294] [total_loss-7.905621]  [loss0-1.040552]'<P>\n",
    "'[step-00000660] [time-215222.316] [total_loss-7.604410]  [loss0-1.002543]'<P>\n",
    "'[step-00000670] [time-218494.370] [total_loss-7.707972]  [loss0-1.024461]'<P>\n",
    "'[step-00000680] [time-221729.626] [total_loss-7.782916]  [loss0-1.043407]'<P>\n",
    "'[step-00000690] [time-225087.905] [total_loss-7.314795]  [loss0-0.975968]'<P>\n",
    "'[step-00000700] [time-228341.462] [total_loss-6.554860]  [loss0-0.871213]'<P>\n",
    "'[step-00000710] [time-231601.378] [total_loss-7.613461]  [loss0-1.014862]'<P>\n",
    "'[step-00000720] [time-234878.777] [total_loss-7.432652]  [loss0-0.985619]'<P>\n",
    "'[step-00000730] [time-238252.022] [total_loss-7.029917]  [loss0-0.932849]'<P>\n",
    "'[step-00000740] [time-241503.330] [total_loss-7.699503]  [loss0-1.023113]'<P>\n",
    "'[step-00000750] [time-244736.616] [total_loss-6.873481]  [loss0-0.909349]'<P>\n",
    "'[step-00000760] [time-247963.514] [total_loss-7.010843]  [loss0-0.923289]'<P>\n",
    "'[step-00000770] [time-251200.339] [total_loss-6.640522]  [loss0-0.897407]'<P>\n",
    "'[step-00000780] [time-254492.488] [total_loss-6.875463]  [loss0-0.913744]'<P>\n",
    "'[step-00000790] [time-257781.725] [total_loss-7.081985]  [loss0-0.941133]'<P>\n",
    "'[step-00000800] [time-261055.921] [total_loss-7.108336]  [loss0-0.934976]'<P>\n",
    "'[step-00000810] [time-264327.337] [total_loss-7.319259]  [loss0-0.979401]'<P>\n",
    "'[step-00000820] [time-267626.805] [total_loss-6.824666]  [loss0-0.914434]'<P>\n",
    "'[step-00000830] [time-270878.371] [total_loss-6.476656]  [loss0-0.871525]'<P>\n",
    "'[step-00000840] [time-274222.778] [total_loss-7.403937]  [loss0-0.979573]'<P>\n",
    "'[step-00000850] [time-277571.363] [total_loss-6.818310]  [loss0-0.915469]'<P>\n",
    "'[step-00000860] [time-280918.615] [total_loss-7.833298]  [loss0-1.059238]'<P>\n",
    "'[step-00000870] [time-284183.712] [total_loss-8.208396]  [loss0-1.088753]'<P>\n",
    "'[step-00000880] [time-287491.948] [total_loss-6.927162]  [loss0-0.931221]'<P>\n",
    "'[step-00000890] [time-290780.453] [total_loss-6.339601]  [loss0-0.840595]'<P>\n",
    "'[step-00000900] [time-294069.817] [total_loss-6.973497]  [loss0-0.932479]'<P>\n",
    "'[step-00000910] [time-297335.374] [total_loss-6.758513]  [loss0-0.901264]'<P>\n",
    "'[step-00000920] [time-300621.774] [total_loss-6.676791]  [loss0-0.900506]'<P>\n",
    "'[step-00000930] [time-303913.596] [total_loss-6.794464]  [loss0-0.896118]'<P>\n",
    "'[step-00000940] [time-307158.881] [total_loss-7.390224]  [loss0-0.999635]'<P>\n",
    "'[step-00000950] [time-310485.134] [total_loss-7.391057]  [loss0-0.991024]'<P>\n",
    "'[step-00000960] [time-313789.019] [total_loss-6.356402]  [loss0-0.847755]'<P>\n",
    "'[step-00000970] [time-317101.714] [total_loss-7.112476]  [loss0-0.935551]'<P>\n",
    "'[step-00000980] [time-320388.355] [total_loss-7.352446]  [loss0-0.986682]'<P>\n",
    "'[step-00000990] [time-323778.970] [total_loss-6.837642]  [loss0-0.917766]'<P>\n",
    "Training done!<P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Model trained on AWS GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nightly",
   "language": "python",
   "name": "pytorch-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
