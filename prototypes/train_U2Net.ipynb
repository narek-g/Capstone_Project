{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922863b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a394d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, utils\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as standard_transforms\n",
    "\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "# import pickle\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b152ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import cv2\n",
    "import pprint\n",
    "import traceback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04015c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5cc53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/Users/narekgeghamyan/Classes/MLE_bootcamp/Capstone_Project')\n",
    "from data.custom_dataset_data_loader import CustomDatasetDataLoader, sample_data\n",
    "from model import U2NET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8484111",
   "metadata": {},
   "source": [
    "### Set-up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9406772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parser(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"training_cloth_segm_u2net_exp1\"  # Expriment name\n",
    "        self.image_folder = \"/Users/narekgeghamyan/local_data/capstone_data/imaterialist-fashion-2019-FGVC6/train\"  # image folder path\n",
    "        self.df_path = \"/Users/narekgeghamyan/local_data/capstone_data/imaterialist-fashion-2019-FGVC6/train684.csv\"  # label csv path\n",
    "        self.distributed = False  # True for multi gpu training\n",
    "        self.isTrain = True\n",
    "\n",
    "        self.fine_width = 192 * 3\n",
    "        self.fine_height = 192 * 3\n",
    "\n",
    "        # Mean std params\n",
    "        self.mean = 0.5\n",
    "        self.std = 0.5\n",
    "\n",
    "        self.batchSize = 2  # 12\n",
    "        self.nThreads = 1 \n",
    "        self.max_dataset_size = float(\"inf\")\n",
    "\n",
    "        self.serial_batches = False\n",
    "        self.continue_train = True\n",
    "        if self.continue_train:\n",
    "            self.unet_checkpoint = \"prev_checkpoints/cloth_segm_unet_surgery.pth\"\n",
    "\n",
    "        self.save_freq = 1000\n",
    "        self.print_freq = 10\n",
    "        self.image_log_freq = 100\n",
    "\n",
    "        self.iter = 3\n",
    "        self.lr = 0.0002\n",
    "        self.clip_grad = 5\n",
    "\n",
    "        self.logs_dir = osp.join(\"logs\", self.name)\n",
    "        self.save_dir = osp.join(\"results\", self.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c527d9",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb1b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bce_loss = nn.BCELoss(reduction='mean')\n",
    "loss_fxn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def loss_fuction(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "\n",
    "\tloss0 = loss_fxn(d0,labels_v)\n",
    "\tloss1 = loss_fxn(d1,labels_v)\n",
    "\tloss2 = loss_fxn(d2,labels_v)\n",
    "\tloss3 = loss_fxn(d3,labels_v)\n",
    "\tloss4 = loss_fxn(d4,labels_v)\n",
    "\tloss5 = loss_fxn(d5,labels_v)\n",
    "\tloss6 = loss_fxn(d6,labels_v)\n",
    "\n",
    "\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n",
    "\n",
    "\treturn loss0, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8868393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaefe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([1, 1.5, 1.5, 1.5], dtype=np.float32)\n",
    "weights = torch.from_numpy(weights)\n",
    "loss_IoU = IoULoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f74299",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54ddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "model_name = 'u2net'\n",
    "if(model_name=='u2net'):\n",
    "#     net = U2NET(3, 1)\n",
    "    net = U2NET(in_ch=3, out_ch=4)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "    print('Cuda available..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8d823",
   "metadata": {},
   "source": [
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f46f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cb14f",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2ca105",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 3\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "save_frq = 3 # save every 2 iterations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f098055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(opt):\n",
    "    \n",
    "    custom_dataloader = CustomDatasetDataLoader()\n",
    "    custom_dataloader.initialize(opt)\n",
    "    loader = custom_dataloader.get_loader()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    local_rank = 0\n",
    "    \n",
    "    net.train()\n",
    "    get_data = sample_data(loader)\n",
    "    for epoch in range(0, epoch_num):\n",
    "\n",
    "        data_batch = next(get_data)\n",
    "        image, label = data_batch\n",
    "        image = Variable(image.to(device))\n",
    "        label = label.type(torch.long)\n",
    "        label = Variable(label.to(device))\n",
    "\n",
    "        print(image)\n",
    "\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "    #     inputs, labels = data['image'], data['label']\n",
    "\n",
    "    #     inputs = inputs.type(torch.FloatTensor)\n",
    "    #     labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "    #     # wrap them in Variable\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "    #                                                                                     requires_grad=False)\n",
    "    #     else:\n",
    "    #         inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6 = net(image)\n",
    "        loss2, loss = loss_fuction(d0, d1, d2, d3, d4, d5, d6, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss #loss.data.item()\n",
    "        running_tar_loss += loss2 #loss2.data.item()\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "\n",
    "        if ite_num % save_frq == 0:\n",
    "\n",
    "            torch.save(net.state_dict(), model_dir + model_name+\"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "            running_loss = 0.0\n",
    "            running_tar_loss = 0.0\n",
    "            net.train()  # resume train\n",
    "            ite_num4val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser()\n",
    "training_loop(opt)\n",
    "print(\"Completed..............\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397aafcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2179b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nightly",
   "language": "python",
   "name": "pytorch-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
